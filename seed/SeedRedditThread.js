// SeedRedditThread.js - VersiÃ³n 2.16 (CORRECCIÃ“N: Centraliza Scrapeo de Link/DescripciÃ³n en Post Seeder)
// USO: node SeedRedditThread.js
// USO PROGRAMADO: node SeedRedditThread.js --scheduled

require('dotenv').config({ path: '../.env' });
const mongoose = require('mongoose');
const connectDB = require('../db');
const Post = require('../models/Post');
const Profile = require('../models/Profile');
const ProfileLike = require('../models/ProfileLike');
const axios = require('axios');
const cheerio = require('cheerio');
const crypto = require('crypto');
const { recordActivityHit } = require('../utils/recordStatsActivity'); 

const REDDIT_CLIENT_ID = process.env.REDDIT_CLIENT_ID;
const REDDIT_CLIENT_SECRET = process.env.REDDIT_CLIENT_SECRET;
const POST_LIMIT = process.env.TRENDING_LIMIT || 500;
const MIN_COMMENTS = process.env.MIN_COMMENTS || 50;

// --- ESTRATEGIA DE BATCHING PARA CONTADORES DE PERFILES ---
const profileUpdatesMap = new Map(); // Mapa para acumular { profileId: { likes: N } }
const TIMEOUT_MS = 25000;
// -----------------------------------------------------------

// Subreddits de tecnologÃ­a/programaciÃ³n a monitorear
const TECH_SUBREDDITS = [
Â  Â  'programming', 'technology', 'computerscience', 'coding', 
Â  Â  'webdev', 'learnprogramming', 'compsci', 'softwareengineering',
Â  Â  'artificial', 'MachineLearning', 'datascience', 'python',
Â  Â  'javascript', 'java', 'cpp', 'golang', 'rust', 'php',
Â  Â  'reactjs', 'node', 'vuejs', 'angular', 'django', 'flask',
Â  Â  'devops', 'sysadmin', 'cybersecurity', 'networking',
Â  Â  'apple', 'android', 'windows', 'linux', 'macos'
];

let accessToken = null;

// --- FUNCIONES DE SCRAPING (NUEVAS / MOVIDAS) ---

const decodeHtmlEntities = (str) => str ? str.replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>') : str;

/**
 * ğŸ†• NUEVA FUNCIÃ“N: Raspa el HTML del permalink de Reddit para encontrar la URL externa.
 */
async function scrapeRedditForExternalLink(redditPermalink) {
Â  Â  try {
Â  Â  Â  Â  console.log(`ğŸ” Scrapeando HTML de Reddit para link externo: ${redditPermalink}`);
Â  Â  Â  Â  const { data } = await axios.get(redditPermalink, { headers: { 'User-Agent': 'TechPosts-Importer/2.16' }, timeout: TIMEOUT_MS });
Â  Â  Â  Â  const $ = cheerio.load(data);
Â  Â  Â  Â  
Â  Â  Â  Â  // Selector basado en la estructura de 'faceplate-tracker'
Â  Â  Â  Â  const selector = 'faceplate-tracker a[target="_blank"][rel*="noopener"][rel*="nofollow"][class*="border-solid"]';

Â  Â  Â  Â  const externalAnchor = $(selector).first();
Â  Â  Â  Â  
Â  Â  Â  Â  if (externalAnchor.length > 0) {
Â  Â  Â  Â  Â  Â  const externalHref = externalAnchor.attr('href');
Â  Â  Â  Â  Â  Â  console.log(`âœ… Link externo encontrado en el HTML de Reddit: ${externalHref}`);
Â  Â  Â  Â  Â  Â  return externalHref;
Â  Â  Â  Â  }

Â  Â  Â  Â  return null;
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error(`âŒ Error al intentar scrapear el permalink de Reddit: ${error.message}`);
Â  Â  Â  Â  return null;
Â  Â  }
}


/**
 * ğŸ†• FUNCIÃ“N MOVIDA/MEJORADA: ImplementaciÃ³n de scraping de la URL de destino para la descripciÃ³n.
 */
async function scrapeWebpage(url) {
Â  Â  try {
Â  Â  Â  Â  console.log(`ğŸŒ Intentando scrapeo de la descripciÃ³n de: ${url}`);
Â  Â  Â  Â  const { data } = await axios.get(url, { headers: { 'User-Agent': 'TechPosts-Importer/2.16' }, timeout: TIMEOUT_MS });
Â  Â  Â  Â  const $ = cheerio.load(data);
Â  Â  Â  Â  
Â  Â  Â  Â  // 1. Buscar descripciÃ³n en meta tags (preferido)
Â  Â  Â  Â  let description = $('meta[name="description"]').attr('content') 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  || $('meta[property="og:description"]').attr('content') 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  || '';
Â  Â  Â  Â  
Â  Â  Â  Â  // 2. Si no hay meta description, intentar obtener el primer pÃ¡rrafo (general)
Â  Â  Â  Â  if (!description) {
Â  Â  Â  Â  Â  Â  const firstParagraph = $('p').first().text();
Â  Â  Â  Â  Â  Â  if (firstParagraph && firstParagraph.length > 50) {
Â  Â  Â  Â  Â  Â  Â  Â  description = firstParagraph.substring(0, 300) + '...'; // Limitar a 300 caracteres
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  return decodeHtmlEntities(description) || '';
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error(`âš ï¸ Error scraping descripciÃ³n de ${url}: ${error.message}`);
Â  Â  Â  Â  return '';
Â  Â  }
}


// --- FUNCIONES DE BATCHING Y REDDIT API (User-Agent actualizado) ---

function accumulateProfileChanges(profileId, changes) {
Â  Â  const current = profileUpdatesMap.get(profileId.toString()) || { likes: 0 };
Â  Â  profileUpdatesMap.set(profileId.toString(), {
Â  Â  Â  Â  likes: current.likes + (changes.likes || 0)
Â  Â  });
}

async function bulkUpdateProfileCounters() {
Â  Â  if (profileUpdatesMap.size === 0) return;

Â  Â  console.log(`â³ Iniciando actualizaciÃ³n en lote para ${profileUpdatesMap.size} perfiles...`);
Â  Â  const bulkOps = [];
Â  Â  
Â  Â  for (const [profileId, changes] of profileUpdatesMap.entries()) {
Â  Â  Â  Â  const update = {};
Â  Â  Â  Â  if (changes.likes > 0) update.likesCount = changes.likes;

Â  Â  Â  Â  if (Object.keys(update).length > 0) {
Â  Â  Â  Â  Â  Â  bulkOps.push({
Â  Â  Â  Â  Â  Â  Â  Â  updateOne: {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filter: { _id: profileId },
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  update: { $inc: update, $set: { updated_at: new Date() } }
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  }
Â  Â  }

Â  Â  if (bulkOps.length > 0) {
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  const result = await Profile.bulkWrite(bulkOps);
Â  Â  Â  Â  Â  Â  console.log(`âœ… ActualizaciÃ³n en lote completada: ${result.modifiedCount} perfiles actualizados.`);
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  Â  console.error(`âŒ Error en la actualizaciÃ³n en lote de contadores:`, error.message);
Â  Â  Â  Â  }
Â  Â  }
}

async function getRedditAccessToken() {
Â  Â  try {
Â  Â  Â  Â  console.log('ğŸ”‘ Obteniendo token de acceso de Reddit...');
Â  Â  Â  Â  const auth = Buffer.from(`${REDDIT_CLIENT_ID}:${REDDIT_CLIENT_SECRET}`).toString('base64');
Â  Â  Â  Â  const response = await axios.post('https://www.reddit.com/api/v1/access_token',
Â  Â  Â  Â  Â  Â  'grant_type=client_credentials', {
Â  Â  Â  Â  Â  Â  Â  Â  headers: {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Authorization': `Basic ${auth}`,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Content-Type': 'application/x-www-form-urlencoded',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'User-Agent': 'TechPosts-Importer/2.16'
Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  timeout: 10000
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  );
Â  Â  Â  Â  accessToken = response.data.access_token;
Â  Â  Â  Â  console.log('âœ… Token de acceso obtenido');
Â  Â  Â  Â  return accessToken;
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error('âŒ Error obteniendo token:', error.response?.data || error.message);
Â  Â  Â  Â  throw error;
Â  Â  }
}

async function makeRedditRequest(url) {
Â  Â  if (!accessToken) {
Â  Â  Â  Â  await getRedditAccessToken();
Â  Â  }
Â  Â  
Â  Â  try {
Â  Â  Â  Â  const response = await axios.get(url, {
Â  Â  Â  Â  Â  Â  headers: {
Â  Â  Â  Â  Â  Â  Â  Â  'Authorization': `Bearer ${accessToken}`,
Â  Â  Â  Â  Â  Â  Â  Â  'User-Agent': 'TechPosts-Importer/2.16'
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  timeout: 15000
Â  Â  Â  Â  });
Â  Â  Â  Â  return response.data;
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error('âŒ Error en solicitud a Reddit:', error.message);
Â  Â  Â  Â  if (error.response?.status === 401) {
Â  Â  Â  Â  Â  Â  console.log('ğŸ”„ Token expirado, obteniendo nuevo...');
Â  Â  Â  Â  Â  Â  await getRedditAccessToken();
Â  Â  Â  Â  Â  Â  return makeRedditRequest(url);
Â  Â  Â  Â  }
Â  Â  Â  Â  throw error;
Â  Â  }
}

/**
Â * Obtiene posts populares de tecnologÃ­a con mÃ­nimo de comentarios
Â */
async function fetchTechPostsWithComments() {
Â  Â  try {
Â  Â  Â  Â  console.log(`ğŸ“¡ Buscando posts de tecnologÃ­a con â‰¥ ${MIN_COMMENTS} comentarios...`);
Â  Â  Â  Â  
Â  Â  Â  Â  let allPosts = [];
Â  Â  Â  Â  
Â  Â  Â  Â  for (const subreddit of TECH_SUBREDDITS) {
Â  Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  Â  console.log(`ğŸ” Escaneando r/${subreddit}...`);
Â  Â  Â  Â  Â  Â  Â  Â  const url = `https://oauth.reddit.com/r/${subreddit}/top?t=day&limit=20`;
Â  Â  Â  Â  Â  Â  Â  Â  const data = await makeRedditRequest(url);
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  const posts = data.data.children
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .filter(post => post.data.num_comments >= MIN_COMMENTS) 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .filter(post => !post.data.over_18) 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .map(post => ({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  id: post.data.id,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title: post.data.title,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  subreddit: post.data.subreddit,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  author: post.data.author,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  upvotes: post.data.ups,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  comments: post.data.num_comments,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  created: post.data.created_utc,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  url: `https://reddit.com${post.data.permalink}`, // Permalink de Reddit
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  external_link_api: post.data.url, // URL que la API devuelve (puede ser externa o Reddit)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  image: getPostImage(post.data),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video: getPostVideo(post.data),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  gallery: getPostGallery(post.data),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  media: getPostMedia(post.data),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  description: post.data.selftext || '',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  nsfw: post.data.over_18
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }));
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  console.log(`âœ… r/${subreddit}: ${posts.length} posts con â‰¥ ${MIN_COMMENTS} comentarios`);
Â  Â  Â  Â  Â  Â  Â  Â  allPosts = allPosts.concat(posts);
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  await new Promise(resolve => setTimeout(resolve, 500));
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  Â  Â  Â  console.error(`âŒ Error en r/${subreddit}:`, error.message);
Â  Â  Â  Â  Â  Â  Â  Â  continue;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  const uniquePosts = allPosts.filter((post, index, self) => 
Â  Â  Â  Â  Â  Â  index === self.findIndex(p => p.url === post.url)
Â  Â  Â  Â  ).sort((a, b) => b.comments - a.comments);
Â  Â  Â  Â  
Â  Â  Â  Â  console.log(`ğŸ¯ Total posts Ãºnicos encontrados: ${uniquePosts.length} (â‰¥ ${MIN_COMMENTS} comentarios)`);
Â  Â  Â  Â  return uniquePosts.slice(0, POST_LIMIT); 
Â  Â  Â  Â  
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error('âŒ Error obteniendo posts de tecnologÃ­a:', error.message);
Â  Â  Â  Â  throw error;
Â  Â  }
}

function getPostImage(postData) {
Â  Â  if (postData.preview && postData.preview.images && postData.preview.images.length > 0) {
Â  Â  Â  Â  return postData.preview.images[0].source.url.replace(/&amp;/g, '&');
Â  Â  }
Â  Â  
Â  Â  if (postData.url && (
Â  Â  Â  Â  postData.url.endsWith('.jpg') || postData.url.endsWith('.jpeg') ||
Â  Â  Â  Â  postData.url.endsWith('.png') || postData.url.endsWith('.gif') ||
Â  Â  Â  Â  postData.url.includes('imgur.com') || postData.url.includes('i.redd.it')
Â  Â  )) {
Â  Â  Â  Â  return postData.url;
Â  Â  }
Â  Â  
Â  Â  if (postData.thumbnail && postData.thumbnail.startsWith('http')) {
Â  Â  Â  Â  return postData.thumbnail;
Â  Â  }
Â  Â  
Â  Â  return null;
}

function getPostVideo(postData) {
Â  Â  if (postData.media && postData.media.reddit_video) {
Â  Â  Â  Â  return postData.media.reddit_video.fallback_url;
Â  Â  }
Â  Â  
Â  Â  if (postData.url && (
Â  Â  Â  Â  postData.url.includes('youtube.com') || postData.url.includes('youtu.be') ||
Â  Â  Â  Â  postData.url.includes('vimeo.com') || postData.url.includes('twitch.tv') ||
Â  Â  Â  Â  postData.url.endsWith('.mp4') || postData.url.endsWith('.webm') ||
Â  Â  Â  Â  postData.url.includes('gfycat.com') || postData.url.includes('redgifs.com')
Â  Â  )) {
Â  Â  Â  Â  return postData.url;
Â  Â  }
Â  Â  
Â  Â  return null;
}

function getPostGallery(postData) {
Â  Â  if (postData.is_gallery && postData.media_metadata) {
Â  Â  Â  Â  const galleryImages = [];
Â  Â  Â  Â  for (const [key, item] of Object.entries(postData.media_metadata)) {
Â  Â  Â  Â  Â  Â  if (item.s && item.s.u) {
Â  Â  Â  Â  Â  Â  Â  Â  galleryImages.push(item.s.u.replace(/&amp;/g, '&'));
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  Â  return galleryImages.length > 0 ? galleryImages : null;
Â  Â  }
Â  Â  return null;
}

function getPostMedia(postData) {
Â  Â  return {
Â  Â  Â  Â  image: getPostImage(postData),
Â  Â  Â  Â  video: getPostVideo(postData),
Â  Â  Â  Â  gallery: getPostGallery(postData)
Â  Â  };
}

function hasMediaContent(postData) {
Â  Â  return !!(postData.image || postData.video || postData.gallery);
}

function getPrimaryMediaUrl(postData) {
Â  Â  if (postData.video) return postData.video;
Â  Â  if (postData.image) return postData.image;
Â  Â  if (postData.gallery && postData.gallery.length > 0) return postData.gallery[0];
Â  Â  return null;
}

function generateEntityId(redditUrl) {
Â  Â  return crypto.createHash('sha256')
Â  Â  Â  Â  .update(redditUrl)
Â  Â  Â  Â  .digest('hex')
Â  Â  Â  Â  .substring(0, 24);
}

async function postExists(entityId) {
Â  Â  const existing = await Post.findOne({ entity: entityId });
Â  Â  return !!existing;
}

async function simulatePostLikes(postId, likesCount, allProfileIds) {
Â  Â  if (likesCount <= 0 || allProfileIds.length === 0) {
Â  Â  Â  Â  return [];
Â  Â  }

Â  Â  try {
Â  Â  Â  Â  const profileIdToAuthorMap = new Map(allProfileIds.map(p => [p._id.toString(), p.author]));
Â  Â  Â  Â  
Â  Â  Â  Â  const shuffledLikerPool = [...allProfileIds].sort(() => 0.5 - Math.random());
Â  Â  Â  Â  const numLikesToCreate = Math.min(likesCount, shuffledLikerPool.length);
Â  Â  Â  Â  const selectedLikers = shuffledLikerPool.slice(0, numLikesToCreate);
Â  Â  Â  Â  
Â  Â  Â  Â  const profileLikeDocs = selectedLikers.map(liker => ({ 
Â  Â  Â  Â  Â  Â  profile_id: liker._id, 
Â  Â  Â  Â  Â  Â  fk_id: postId, 
Â  Â  Â  Â  Â  Â  fk_type: 'post' 
Â  Â  Â  Â  }));
Â  Â  Â  Â  
Â  Â  Â  Â  if (profileLikeDocs.length > 0) {
Â  Â  Â  Â  Â  Â  await ProfileLike.insertMany(profileLikeDocs);
Â  Â  Â  Â  Â  Â  console.log(`â¤ï¸ Â ${profileLikeDocs.length} likes simulados para el post ${postId}`);
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  await recordActivityHit(`activity:likes:${process.env.CID}`, 'added', profileLikeDocs.length);
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  const likerAuthors = selectedLikers.map(l => profileIdToAuthorMap.get(l._id.toString()) || l.author);
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  await Post.findByIdAndUpdate(postId, {
Â  Â  Â  Â  Â  Â  Â  Â  $push: { likes: { $each: likerAuthors, $slice: -200 } }
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  console.log(`âœï¸ Â AÃ±adidos ${likerAuthors.length} autores (hashes) al array de likes del post.`);

Â  Â  Â  Â  Â  Â  for (const liker of selectedLikers) {
Â  Â  Â  Â  Â  Â  Â  Â  accumulateProfileChanges(liker._id, { likes: 1 });
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  return likerAuthors;
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  return [];
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error(`âŒ Error simulando likes para post ${postId}:`, error.message);
Â  Â  Â  Â  return [];
Â  Â  }
}

async function importPost(postData, allProfileIds) {
Â  Â  const entityId = generateEntityId(postData.url);
Â  Â  
Â  Â  if (await postExists(entityId)) {
Â  Â  Â  Â  console.log(`â© Post ya existe: r/${postData.subreddit} - ${postData.title.substring(0, 60)}...`);
Â  Â  Â  Â  return { skipped: true, reason: 'exists' };
Â  Â  }
    
    // 1. DETERMINAR URL FINAL Y BUSCAR DESCRIPCIÃ“N
    let finalLink = postData.external_link_api;
    let description = postData.description; // Selftext o vacÃ­o
    
    // Si la URL de la API es el permalink de Reddit (o no es obvia), raspamos el HTML
    if (!finalLink || finalLink.includes('reddit.com')) {
        const scrapedLink = await scrapeRedditForExternalLink(postData.url); // postData.url es el permalink de Reddit
        if (scrapedLink) {
            finalLink = scrapedLink;
        } else {
            // Si el scraping HTML falla, usamos el permalink de Reddit como link final (fallback)
            finalLink = postData.url; 
        }
    }

    // 2. SCRAPING de la pÃ¡gina final para obtener la descripciÃ³n si no es selftext
    if (!postData.description && finalLink && !finalLink.includes('reddit.com')) {
        const scrapedDescription = await scrapeWebpage(finalLink);
        description = scrapedDescription || description; 
    }

    // 3. Control de contenido para posts sin media
    // Solo permitimos posts que tienen media O que tienen un link externo Ãºtil
    if (!hasMediaContent(postData) && finalLink.includes('reddit.com')) {
Â  Â  Â  Â  console.log(`âŒ Post sin multimedia Y sin link externo - SKIPPED: r/${postData.subreddit} - ${postData.title.substring(0, 60)}...`);
Â  Â  Â  Â  return { skipped: true, reason: 'no_media' };
Â  Â  }
Â  Â  
Â  Â  try {
Â  Â  Â  Â  const primaryMedia = getPrimaryMediaUrl(postData);
Â  Â  Â  Â  
Â  Â  Â  Â  const post = new Post({
Â  Â  Â  Â  Â  Â  cid: process.env.CID || 'QU-ME7HF2BN-E8QD9',
Â  Â  Â  Â  Â  Â  entity: entityId,
Â  Â  Â  Â  Â  Â  reference: finalLink, // URL FINAL DEL ARTÃCULO
Â  Â  Â  Â  Â  Â  title: postData.title.substring(0, 100),
Â  Â  Â  Â  Â  Â  description: description.substring(0, 200) || '', // DescripciÃ³n scrapeada/selftext
Â  Â  Â  Â  Â  Â  type: 'reddit_tech',
Â  Â  Â  Â  Â  Â  link: finalLink, // URL FINAL DEL ARTÃCULO
Â  Â  Â  Â  Â  Â  image: primaryMedia, 
Â  Â  Â  Â  Â  Â  media: postData.media, 
Â  Â  Â  Â  Â  Â  likesCount: postData.upvotes,
Â  Â  Â  Â  Â  Â  commentCount: postData.comments,
Â  Â  Â  Â  Â  Â  viewsCount: 0, 
Â  Â  Â  Â  Â  Â  created_at: new Date(postData.created * 1000),
Â  Â  Â  Â  Â  Â  updated_at: new Date(postData.created * 1000),
Â  Â  Â  Â  Â  Â  metadata: {
Â  Â  Â  Â  Â  Â  Â  Â  subreddit: postData.subreddit,
Â  Â  Â  Â  Â  Â  Â  Â  author: postData.author,
Â  Â  Â  Â  Â  Â  Â  Â  nsfw: postData.nsfw,
Â  Â  Â  Â  Â  Â  Â  Â  original_comments: postData.comments,
Â  Â  Â  Â  Â  Â  Â  Â  imported_comments: false,
                reddit_permalink: postData.url, // Guardamos el permalink de Reddit como metadata
Â  Â  Â  Â  Â  Â  Â  Â  has_image: !!postData.image,
Â  Â  Â  Â  Â  Â  Â  Â  has_video: !!postData.video,
Â  Â  Â  Â  Â  Â  Â  Â  has_gallery: !!postData.gallery,
Â  Â  Â  Â  Â  Â  Â  Â  media_count: postData.gallery ? postData.gallery.length : 0
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  });
Â  Â  Â  Â  
Â  Â  Â  Â  await post.save();
Â  Â  Â  Â  console.log(`âœ… Post importado: r/${postData.subreddit} (Link: ${finalLink.substring(0, 40)}...)`);
Â  Â  Â  Â  
Â  Â  Â  Â  if (postData.upvotes > 0 && allProfileIds.length > 0) {
Â  Â  Â  Â  Â  Â  await simulatePostLikes(post._id, postData.upvotes, allProfileIds);
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  return { success: true, post };
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error(`âŒ Error importando post:`, error.message);
Â  Â  Â  Â  return { error: true };
Â  Â  }
}

function getMediaType(postData) {
Â  Â  if (postData.video) return 'video';
Â  Â  if (postData.gallery) return `gallery(${postData.gallery.length} images)`;
Â  Â  if (postData.image) return 'image';
Â  Â  return 'no media';
}

async function runImportProcess() {
Â  Â  let exitCode = 0;
Â  Â  try {
Â  Â  Â  Â  if (!REDDIT_CLIENT_ID || !REDDIT_CLIENT_SECRET) {
Â  Â  Â  Â  Â  Â  throw new Error('âŒ Credenciales de Reddit no configuradas en .env');
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  await connectDB();
Â  Â  Â  Â  console.log('âœ… Conectado a la base de datos');
Â  Â  Â  Â  
Â  Â  Â  Â  console.log('ğŸ‘¤ Obteniendo IDs y Autores de perfiles para simulaciÃ³n de likes...');
Â  Â  Â  Â  const allProfileIds = await Profile.find({}, '_id author').lean(); 
Â  Â  Â  Â  console.log(`ğŸ‘ Encontrados ${allProfileIds.length} perfiles para usar como votantes.`);
Â  Â  Â  Â  
Â  Â  Â  Â  const techPosts = await fetchTechPostsWithComments();
Â  Â  Â  Â  
Â  Â  Â  Â  console.log(`\nğŸ“¥ Analizando y filtrando posts para importar...`);
Â  Â  Â  Â  
Â  Â  Â  Â  let imported = 0;
Â  Â  Â  Â  let skippedExists = 0;
Â  Â  Â  Â  let skippedNoMedia = 0;
Â  Â  Â  Â  let errors = 0;
Â  Â  Â  Â  
Â  Â  Â  Â  for (const post of techPosts) {
Â  Â  Â  Â  Â  Â  const result = await importPost(post, allProfileIds);
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  if (result.skipped) {
Â  Â  Â  Â  Â  Â  Â  Â  if (result.reason === 'no_media') {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  skippedNoMedia++;
Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  skippedExists++;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  } else if (result.success) {
Â  Â  Â  Â  Â  Â  Â  Â  imported++;
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  errors++;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  await new Promise(resolve => setTimeout(resolve, 200));
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  await bulkUpdateProfileCounters(); 
Â  Â  Â  Â  
Â  Â  Â  Â  console.log(`\nğŸ‰ ImportaciÃ³n completada:`);
Â  Â  Â  Â  console.log(` Â  âœ… Nuevos posts: ${imported}`);
Â  Â  Â  Â  console.log(` Â  â© Ya existÃ­an: ${skippedExists}`);
Â  Â  Â  Â  console.log(` Â  ğŸš« Sin link Ãºtil/media (omitidos): ${skippedNoMedia}`);
Â  Â  Â  Â  console.log(` Â  âŒ Errores: ${errors}`);
Â  Â  Â  Â  console.log(` Â  ğŸ“Š Total analizados: ${techPosts.length}`);
Â  Â  Â  Â  
Â  Â  } catch (error) {
Â  Â  Â  Â  console.error('âŒ Error en importaciÃ³n:', error.message);
Â  Â  Â  Â  exitCode = 1;
Â  Â  } finally {
Â  Â  Â  Â  await mongoose.connection.close();
Â  Â  Â  Â  console.log('âœ… ConexiÃ³n cerrada');
Â  Â  Â  Â  return exitCode;
Â  Â  }
}

async function main() {
Â  Â  console.log('ğŸš€ Iniciando importaciÃ³n de posts de tecnologÃ­a...');
Â  Â  console.log(`â° Hora de inicio: ${new Date().toISOString()}`);
Â  Â  
Â  Â  const exitCode = await runImportProcess();
Â  Â  
Â  Â  console.log(`\nğŸ‰ Proceso finalizado.`);
Â  Â  process.exit(exitCode);
}

async function scheduledExecution() {
Â  Â  const INTERVAL_MS = 60 * 60 * 1000; 
Â  Â  console.log(`\nâ° Iniciando ciclo de ejecuciÃ³n programada (cada ${INTERVAL_MS / 1000 / 60} minutos)...`);

Â  Â  const executeCycle = async () => {
Â  Â  Â  Â  console.log(`\n--- EjecuciÃ³n de importaciÃ³n de posts ---`);
Â  Â  Â  Â  console.log(`â° Hora de inicio: ${new Date().toISOString()}`);
Â  Â  Â  Â  
Â  Â  Â  Â  const exitCode = await runImportProcess(); 
Â  Â  Â  Â  
Â  Â  Â  Â  const nextRun = new Date(Date.now() + INTERVAL_MS);
Â  Â  Â  Â  console.log(`â­ï¸ Â PrÃ³xima ejecuciÃ³n: ${nextRun.toISOString()}`);
Â  Â  Â  Â  
Â  Â  Â  Â  setTimeout(executeCycle, INTERVAL_MS);
Â  Â  };
Â  Â  
Â  Â  executeCycle();
}

if (require.main === module) {
Â  Â  if (process.argv.includes('--scheduled')) {
Â  Â  Â  Â  scheduledExecution();
Â  Â  } else {
Â  Â  Â  Â  main().catch(console.error);
Â  Â  }
}

module.exports = { 
Â  Â  runImportProcess, 
Â  Â  main, 
Â  Â  scheduledExecution 
};